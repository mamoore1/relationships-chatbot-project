{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practicing using NLTK by creating a retrieval-based chatbot. The chatbot responses will be drawn from a selection of reddit comments taken from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subreddit                                               body  \\\n",
      "0  gameofthrones  Your submission has been automatically removed...   \n",
      "1            aww  Dont squeeze her with you massive hand, you me...   \n",
      "2         gaming  It's pretty well known and it was a paid produ...   \n",
      "3           news  You know we have laws against that currently c...   \n",
      "4       politics  Yes, there is a difference between gentle supp...   \n",
      "\n",
      "   controversiality  score  \n",
      "0                 0      1  \n",
      "1                 0     19  \n",
      "2                 0      3  \n",
      "3                 0     10  \n",
      "4                 0      1  \n",
      "1000000\n",
      "40\n",
      "['gameofthrones' 'aww' 'gaming' 'news' 'politics' 'dankmemes'\n",
      " 'relationship_advice' 'nba' 'worldnews' 'AskReddit' 'AmItheAsshole'\n",
      " 'SquaredCircle' 'The_Donald' 'leagueoflegends' 'hockey' 'videos'\n",
      " 'teenagers' 'gonewild' 'movies' 'funny' 'pics' 'marvelstudios' 'memes'\n",
      " 'soccer' 'freefolk' 'MortalKombat' 'todayilearned' 'apexlegends' 'asoiaf'\n",
      " 'Market76' 'Animemes' 'FortNiteBR' 'nfl' 'trashy' 'unpopularopinion'\n",
      " 'ChapoTrapHouse' 'RoastMe' 'Showerthoughts' 'wallstreetbets' 'Pikabu']\n"
     ]
    }
   ],
   "source": [
    "reddit_text_df = pd.read_csv('kaggle_RC_2019-05.csv')\n",
    "\n",
    "# Examining data\n",
    "print(reddit_text_df.head())\n",
    "print(len(reddit_text_df))\n",
    "print(reddit_text_df.subreddit.nunique())\n",
    "print(reddit_text_df.subreddit.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains 1,000,000 reddit posts drawn from 40 subreddits. This is probably too much data for our chatbot to work reasonably quickly; further, some of these subreddits contain a lot of offensive content. I will therefore choose to use responses taken solely from r/relationship_advice, making this a relationship advice chatbot! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "               subreddit                                               body  \\\n",
      "6    relationship_advice  I would be less worried about how he fucked up...   \n",
      "18   relationship_advice  I would actually just like to say that there a...   \n",
      "209  relationship_advice  Do you find it relevant when it happens or if ...   \n",
      "300  relationship_advice  Don't bother giving her the power to make a ch...   \n",
      "342  relationship_advice  If the relationship is not wholly fulfilling, ...   \n",
      "\n",
      "     controversiality  score  \n",
      "6                   0      7  \n",
      "18                  0      0  \n",
      "209                 0      1  \n",
      "300                 0      2  \n",
      "342                 0      1  \n",
      "I would be less worried about how he fucked up in the past and more worried that I still can't get a straight answer about it now.\n"
     ]
    }
   ],
   "source": [
    "relad_df = reddit_text_df[reddit_text_df['subreddit'] == 'relationship_advice']\n",
    "\n",
    "print(len(relad_df))\n",
    "print(relad_df.head())\n",
    "print(relad_df.body.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25000 responses, especially since they are not single sentences, is far too many responses, so we choose only the first 1000. We next transfer the responses from the Pandas Dataframe into a list and then filter for offensive terms, dropping any responses which contain such terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I would be less worried about how he fucked up in the past and more worried that I still can't get a straight answer about it now.\", \"I would actually just like to say that there are other possibilities at play here.\\n\\nI laughed when I read this, because just today I downloaded tinder because we were discussing dating at our age, and the single people were saying  it's difficult to find cool people.    So I downloaded it, swiped through, and deleted all within 10 minutes.\\n\\nI have 0 interest in other woman, and I love my wife, and there's no way I'm cheating on her....but I still downloaded it.   Btw my wife knows, and thinks it's harmless fun.   \\n\\n So anyway, she might be cheating, but no need to go crazy just yet.\", 'Do you find it relevant when it happens or if the topic happens to come up?', \"Don't bother giving her the power to make a choice. Walk away from her and never look back that way you can keep your dignity! You will thank yourself later in life for standing up for yourself\", \"If the relationship is not wholly fulfilling, it's probably a sign that the two of you aren't meant to be together. Also you are only 18, so while it could be possible, she is likely not the one. People who get together at a young age often don't end up staying together forever. Some do, but I would say, based on what you've said, it doesn't sound like it. Before you said you broke up with her because it didn't feel right, so you were likely right then. What you need to do is sit yourself down and think about whether or not you can continue this relationship. Can the good outweigh the bad or is there too much bad? I can't speak to it, obviously, because I am not you. Ultimately, you need to figure out if you can continue on in this relationship or not.\", \"first off, why would you want to be with someone who's considering you second best? lol be done with her and move on. There's always going to be someone else better out there for her and you're just temporary till that happens\", 'Incest fetishes are suuuuuper common, I think people just tend to see them as more ‘normal’ in men and more something that indicates damage in women, which is bullshit.\\n\\nThat said, just because it’s not gross/weird imho doesn’t mean you have to engage in something you’re not comfortable with, and it’s totally understandable why that’s the case. \\n\\nI would explain all of this to her the same way you did here, and see if she has any alternative ideas at how you could approach it, but otherwise she’s going to have to drop it because pressuring you into sex that makes you uncomfortable should be pretty much game over for the relationship.', 'Part of being in a relationship is getting through the tough stuff. If you think he’s worth it, then rich or poor, hang in there. Just find a safe place to vent, someone that won’t judge him or your relationship unless he is mistreating you personally.', '*multiple times????* also did the guy know your gf was in a relationship? you should beat the shit out of him he he had sex with your gf knowingly.', 'Yes but i was asking what actual evidence you have besides just \"knowing\" im not presuming things about your relationship but people think they know the truth all the time when the reality is quite different.']\n"
     ]
    }
   ],
   "source": [
    "text = [response for response in relad_df.body.iloc[0:100]]\n",
    "print(text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "[\"I would actually just like to say that there are other possibilities at play here.\\n\\nI laughed when I read this, because just today I downloaded tinder because we were discussing dating at our age, and the single people were saying  it's difficult to find cool people.    So I downloaded it, swiped through, and deleted all within 10 minutes.\\n\\nI have 0 interest in other woman, and I love my wife, and there's no way I'm cheating on her....but I still downloaded it.   Btw my wife knows, and thinks it's harmless fun.   \\n\\n So anyway, she might be cheating, but no need to go crazy just yet.\", 'Do you find it relevant when it happens or if the topic happens to come up?']\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "for item in text:\n",
    "    if re.search(\"cu+nt|\\w*shi+t\\w*|fu+ck\\w*|ni+gg\\w*\", item.lower()):\n",
    "        continue\n",
    "    else:\n",
    "        clean_text.append(item)\n",
    "\n",
    "print(len(clean_text))\n",
    "print(clean_text[0:2])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With profanity removed and the text placed in a list, I will now begin processing the text for use with the chatbot. We will be using NLTKs TfIdfVectorizer and cosine similarity to determine which response would be most appropriate for the chatbot. However, as we want to keep the original version of the responses so that they can be returned by the chatbot, we will separate the data between it's original version and its lemmatized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i would actually just like to say that there are other possibilities at play here\\n\\ni laughed when i read this because just today i downloaded tinder because we were discussing dating at our age and the single people were saying  its difficult to find cool people    so i downloaded it swiped through and deleted all within 10 minutes\\n\\ni have 0 interest in other woman and i love my wife and theres no way im cheating on herbut i still downloaded it   btw my wife knows and thinks its harmless fun   \\n\\n so anyway she might be cheating but no need to go crazy just yet', 'do you find it relevant when it happens or if the topic happens to come up']\n",
      "[[('would', 'n'), ('actually', 'r'), ('like', 'v'), ('say', 'n'), ('possibilities', 'n'), ('play', 'v'), ('laughed', 'v'), ('read', 'v'), ('today', 'n'), ('downloaded', 'v'), ('tinder', 'n'), ('discussing', 'v'), ('dating', 'v'), ('age', 'n'), ('single', 'a'), ('people', 'n'), ('saying', 'v'), ('difficult', 'a'), ('find', 'a'), ('cool', 'a'), ('people', 'n'), ('downloaded', 'v'), ('swiped', 'v'), ('deleted', 'v'), ('within', 'n'), ('10', 'n'), ('minutes', 'n'), ('0', 'n'), ('interest', 'n'), ('woman', 'n'), ('love', 'v'), ('wife', 'n'), ('theres', 'n'), ('way', 'n'), ('im', 'n'), ('cheating', 'v'), ('herbut', 'n'), ('still', 'r'), ('downloaded', 'v'), ('btw', 'a'), ('wife', 'n'), ('knows', 'v'), ('thinks', 'v'), ('harmless', 'a'), ('fun', 'n'), ('anyway', 'r'), ('might', 'n'), ('cheating', 'v'), ('need', 'v'), ('go', 'v'), ('crazy', 'a'), ('yet', 'r')], [('find', 'v'), ('relevant', 'a'), ('happens', 'n'), ('topic', 'n'), ('happens', 'v'), ('come', 'v')]]\n"
     ]
    }
   ],
   "source": [
    "# Using string.punctuation to remove punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    punctuation_dictionary = dict((ord(item), None) for item in string.punctuation)\n",
    "    new_text = [item.lower().translate(punctuation_dictionary) for item in text]\n",
    "    return new_text\n",
    "    \n",
    "filtered_text = remove_punct(clean_text)\n",
    "print(filtered_text[0:2])\n",
    "\n",
    "# Using NLKT to tokenize the responses,remove stopwords and tag the parts of speech (for more accurate lemmatization)\n",
    "\n",
    "stop_words = stopwords.words('english') \n",
    "\n",
    "# Function to translate pos_tag tags into the form needed for WordNet\n",
    "\n",
    "def get_wordnet_pos(sentence):\n",
    "    tag_list = []\n",
    "    for i in range(len(sentence)):\n",
    "        tag = pos_tag(sentence)[i][1][0].upper()\n",
    "        tag_dict = {\n",
    "            \"R\": wordnet.ADV,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ\n",
    "        }\n",
    "        tag = tag_dict.get(tag, wordnet.NOUN)\n",
    "        tag_list.append((sentence[i], tag))    \n",
    "    return tag_list\n",
    "\n",
    "# Determining parts of speech and converting into the appropriate form for WordNet\n",
    "def pos_words(text):\n",
    "    new_text = []\n",
    "    for item in text: # Accessing each response\n",
    "        new_item = []\n",
    "        for word in word_tokenize(item): # Tokenising each response\n",
    "            if word not in stop_words: # Checking the tokenised words against stopwords\n",
    "                new_item.append(word)\n",
    "        pos_new_item = get_wordnet_pos(new_item) # Tagging the part of speech for each word\n",
    "        new_text.append(pos_new_item)\n",
    "    return new_text\n",
    "\n",
    "filtered_text = pos_words(filtered_text)\n",
    "print(filtered_text[0:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['would', 'actually', 'like', 'say', 'possibility', 'play', 'laugh', 'read', 'today', 'download', 'tinder', 'discuss', 'date', 'age', 'single', 'people', 'say', 'difficult', 'find', 'cool', 'people', 'download', 'swipe', 'delete', 'within', '10', 'minute', '0', 'interest', 'woman', 'love', 'wife', 'there', 'way', 'im', 'cheat', 'herbut', 'still', 'download', 'btw', 'wife', 'know', 'think', 'harmless', 'fun', 'anyway', 'might', 'cheat', 'need', 'go', 'crazy', 'yet'], ['find', 'relevant', 'happens', 'topic', 'happen', 'come']]\n",
      "['would actually like say possibility play laugh read today download tinder discuss date age single people say difficult find cool people download swipe delete within 10 minute 0 interest woman love wife there way im cheat herbut still download btw wife know think harmless fun anyway might cheat need go crazy yet', 'find relevant happens topic happen come', 'dont bother give power make choice walk away never look back way keep dignity thank later life stand']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing our tokenized words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(pos_text):\n",
    "    new_text = []\n",
    "    for response in pos_text:\n",
    "        new_response = []\n",
    "        for word_tag in response:\n",
    "            new_response.append(lemmatizer.lemmatize(word_tag[0], word_tag[1]))\n",
    "        new_text.append(new_response)\n",
    "    return new_text\n",
    "\n",
    "lemma_text = lemmatize_text(filtered_text)\n",
    "print(lemma_text[0:2])\n",
    "\n",
    "lemma_text = [\" \".join(word) for word in lemma_text[0:3]]\n",
    "print(lemma_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would actually like say possibility play laugh read today download tinder discuss date age single people say difficult find cool people download swipe delete within 10 minute 0 interest woman love wife there way im cheat herbut still download btw wife know think harmless fun anyway might cheat need go crazy yet', 'find relevant happens topic happen come']\n"
     ]
    }
   ],
   "source": [
    "# Combining the previous steps into one function which can tokenise and lemmatize text\n",
    "def lemmafunction(text):\n",
    "    de_punct_text = remove_punct(text)\n",
    "    pos_text = pos_words(de_punct_text)\n",
    "    lemmad_text = lemmatize_text(pos_text)\n",
    "    lemma_response = [\" \".join(response) for response in lemmad_text]\n",
    "    return lemma_response\n",
    "\n",
    "print(lemmafunction(clean_text[0:2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a lemmatizing function built, I can now build a Tfidf Vectorizer using Scikit-Learn. This vectorizer will take in the user response and our possible replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Building Tfidf Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidfVec = TfidfVectorizer(tokenizer=lemmafunction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need on the vectorizing side. Now I will build the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the relationship advice chatbot!\n",
      "Are you able to chat now? (y/n)\n",
      "> y\n",
      "\n",
      "Ask me a question about relationships!\n",
      "> exit\n",
      "Goodbye then!\n"
     ]
    }
   ],
   "source": [
    "import random # using random so that it will not always be the same response\n",
    "\n",
    "class Chatbot():\n",
    "    def __init__(self):\n",
    "        self.exit_commands = ['end', 'exit', 'goodbye', 'quit', 'stop']\n",
    "        \n",
    "    # Function to handle conversation opening with user    \n",
    "    def intro(self):\n",
    "        print('Welcome to the relationship advice chatbot!')\n",
    "        user_input = input('Are you able to chat now? (y/n)\\n> ').lower()\n",
    "        if user_input in ['y', 'yes', 'yeah']:\n",
    "            self.chatting()\n",
    "        else:\n",
    "            print('Okay, talk to you later!')\n",
    "    \n",
    "    # Function to handle general conversation\n",
    "    def chatting(self):\n",
    "        print('\\nAsk me a question about relationships!')\n",
    "        user_input = input('> ')\n",
    "        while user_input not in self.exit_commands:\n",
    "            user_input = input(self.generate_response(user_input))\n",
    "        print('Goodbye then!')\n",
    "    \n",
    "    def generate_response(self, user_input):\n",
    "        chatbot_response = ''\n",
    "        # Appending the user response to the end of our cleaned text\n",
    "        new_text = ''\n",
    "        new_text = clean_text + [user_input]\n",
    "        tfidf_text = tfidfVec.fit_transform(new_text)\n",
    "        \n",
    "        #identifying idx of best match\n",
    "        values = cosine_similarity(tfidf_text[-1], tfidf_text) # Comparing similarities\n",
    "        random_idx = random.choice(range(-2, -6, -1))\n",
    "        idx = values.argsort()[0][random_idx]\n",
    "        flat = values.flatten()\n",
    "        req_tfidf = flat[random_idx]\n",
    "        print('req_tfidf: ', req_tfidf, idx)\n",
    "        if req_tfidf == 0:\n",
    "            chatbot_response = 'Sorry, I can\\'t help you.'\n",
    "            return chatbot_response + '\\n> '\n",
    "        else:\n",
    "            chatbot_response += clean_text[idx]\n",
    "            return chatbot_response +'\\n> '\n",
    "        \n",
    "        \n",
    "    \n",
    "chatbot = Chatbot()\n",
    "chatbot.intro()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
